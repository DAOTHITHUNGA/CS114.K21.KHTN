{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm_Detec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAOTHITHUNGA/CS114.K21.KHTN/blob/master/Sarcasm_Detec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s17LtCPKXLr9",
        "colab_type": "text"
      },
      "source": [
        "###BÁO CÁO BÀI TẬP LỚN \n",
        "Đào Thị Thu Nga - 18521135\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBtnNO1PKy2I",
        "colab_type": "text"
      },
      "source": [
        "Mô tả bài toán : \n",
        "* data : Lấy headlines ở 2 trang chính là:\n",
        "+ huffpost : chứa các tin không châm biếm \n",
        "+ TheOnion : chứa các tin châm biếm (is sarcastic)\n",
        "* Gồm 3 thuộc tính: \n",
        "  + article_link : đường link dẫn tới bài báo \n",
        "  + is_sarcastic : xét có phải là châm biếm hay không với 2 option ( 1 nếu là châm biếm , 0 nếu là không châm biếm )\n",
        "  + headlines : tên tiêu đề bài báo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oieyp5SOSv7",
        "colab_type": "text"
      },
      "source": [
        "### Từ tiêu đề của một bài báo, dự đoán bài báo đó có mang nội dung cà khịa hay không.\n",
        "\n",
        "#### Cách thu thập dữ diệu:\n",
        "-  Trước đây về Phát hiện Sarcasm chủ yếu sử dụng các bộ dữ liệu Twitter được thu thập bằng cách dựa trên hashtag nhưng các bộ dữ liệu đó rất nhiễu về LABEL và ngôn ngữ. Hơn nữa, nhiều tweet là trả lời các tweet khác và phát hiện sự mỉa mai trong những điều này đòi hỏi phải có sẵn các tweet theo ngữ cảnh. Để khắc phục những hạn chế liên quan đến  nhiễu trong bộ dữ liệu Twitter.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "- Dữ liệu mới được thu thập để giảm độ nhiễu hơn so với dữ liệu thu thập từ Tweet từ trước. Dữ liệu được thu thập từ 2 trang tin tức chính:\n",
        "    - Tập mang tính mỉa mai : Được thu thập từ trang tin tức TheOnion, từ 2 chủ đề chính là News in Brief và News in Photos\n",
        "    - Tập không mang tính mỉa mai : Được thu thập từ trang tin tức HuffPost.\n",
        "- So với dữ liệu cũ, dữ liệu mới có một số cải thiện:\n",
        "    - Được viết bởi những nhà báo chuyên nghiệp, không mắc các lỗi ngữ pháp, có quy chuẩn. Giảm thiểu việc phải tiền xử lý dữ liệu.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLUHnDc6LF4K",
        "colab_type": "text"
      },
      "source": [
        "###Cách lấy dữ liệu từ 2 trang huffpost và TheOnion \n",
        "Lấy file json của trang huffpost \n",
        "  - 1 lần  chỉ lấy được 500 line \n",
        "- Thực hiện các thao tác lấy file json của huffpost \n",
        "  - Lần 1 : Lấy 500 headline ở https://www.huffpost.com/api/department/news/cards?page=1&limit=500\n",
        "  - Lần 2: Lấy 143 headline ở https://www.huffpost.com/api/department/news/cards?page=2&limit=500\n",
        "Lấy dữ liệu từ trang TheOnion \n",
        "+ News in Brief và News in Photos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2yE72d7efJr",
        "colab_type": "text"
      },
      "source": [
        "###CRAWL 2000 DỮ LIỆU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zjMUdatdsbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import thư viện cần thiết \n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdnbTrP7gTCr",
        "colab_type": "text"
      },
      "source": [
        "Xử lí file json của huffpost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff2ji-JtdvJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mở file chứa 158 headlines\n",
        "with open('/content/huffpost.json') as f:\n",
        "    part1 = json.load(f)\n",
        "#mở file chứa 500 headlines\n",
        "with open('/content/huffpost2.json') as f:\n",
        "  part2 = json.load(f)\n",
        "#tạo file data tên huffpost_headlines.\n",
        "csv_file = open('huffpost_headlines.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['article_link','headlines', 'is_sarcastic'])\n",
        "for article in part1[\"cards\"]:\n",
        "  headline = article[\"headlines\"][0][\"text\"]\n",
        "  article_link = article[\"headlines\"][0][\"url\"]\n",
        "  csv_writer.writerow([article_link,headline,0])\n",
        "    \n",
        "for article in part2[\"cards\"]:\n",
        "  headline = article[\"headlines\"][0][\"text\"]\n",
        "  article_link = article[\"headlines\"][0][\"url\"]\n",
        "  csv_writer.writerow([article_link,headline,0])\n",
        "  #csv_file.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTO9q1nfeDL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "04372c55-00e7-47cc-caff-634d8e6c4a08"
      },
      "source": [
        "#đọc file huffpost_headlines.csv\n",
        "huffpost_df = pd.read_csv(\"huffpost_headlines.csv\")\n",
        "huffpost_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headlines</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffpost.com/entry/prince-william-...</td>\n",
              "      <td>Prince William Steps Out To Visit COVID-19 Vac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffpost.com/entry/supreme-court-d...</td>\n",
              "      <td>Supreme Court Bolsters Trump's Power Over Rapi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.huffpost.com/entry/george-floyd-di...</td>\n",
              "      <td>George Floyd Died A Month Ago. There Have Been...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.huffpost.com/entry/carly-fiorina-2...</td>\n",
              "      <td>Former GOP Candidate Carly Fiorina Says She Wo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffpost.com/entry/colorado-elijah...</td>\n",
              "      <td>Colorado Governor Will Examine Death Of Elijah...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  ... is_sarcastic\n",
              "0  https://www.huffpost.com/entry/prince-william-...  ...            0\n",
              "1  https://www.huffpost.com/entry/supreme-court-d...  ...            0\n",
              "2  https://www.huffpost.com/entry/george-floyd-di...  ...            0\n",
              "3  https://www.huffpost.com/entry/carly-fiorina-2...  ...            0\n",
              "4  https://www.huffpost.com/entry/colorado-elijah...  ...            0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgePMWxCgc2Y",
        "colab_type": "text"
      },
      "source": [
        "#Chuyển đổi từ file csv sang json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3JeUoRSeFsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "csvfile = open('/content/huffpost_headlines.csv', 'r')\n",
        "jsonfile = open('file.json', 'w')\n",
        "\n",
        "fieldnames = (\"article_link\",\"headlines\", \"is_sarcastic\")\n",
        "reader = csv.DictReader( csvfile, fieldnames)\n",
        "for row in reader:\n",
        "    json.dump(row, jsonfile)\n",
        "    jsonfile.write('\\n')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59wMMjvugjIF",
        "colab_type": "text"
      },
      "source": [
        "#CRAWL dữ liệu từ TheOnion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8i0VX7geITp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b221cbb-63dc-4c64-c9b2-a00df9f523e9"
      },
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "brief_ids = [1591290240502,1590689280047,1589990700904,1589468640865,1588797540738,1588268460534,\n",
        "             1587475140185,1586891580286,1586290320129,1585667400574,\n",
        "             1584992760722,1584481080367,1583943300489,1583433840399,1583174280261,\n",
        "             1582734480159,1582230180154,1581691680914,1581347940240,\n",
        "              1580846940042,1580238780534,1579722060702,1579199040121,1578667500624,\n",
        "              1576785600904,1576524120524,1575914280494,1575386760500,1574446260671,\n",
        "              1573853280300,1573574280081,1573065060211,1572465120742,1572010200227,\n",
        "              1571684700481,1571189760863,1570808100042,1570464000244,1570017600467,1569521460355,1569256200911\n",
        "              ,1568653560896,1568141220732,1567616220385,1567090800228,1566577200032,\n",
        "              1566249480367,1565716500732,1565282640227,1564762560287,1564500720500,1563984120219,1563541200063\n",
        "             ,1563219960624,1562682300628]\n",
        "\n",
        "photo_ids = [1590159240893,1588856760531,1587659280048,1586191620240,1584377100135,1582815480621,1582135980235,1580919900163,1579630380938\n",
        "            ,1576675800080,1574713020966,1573241220619,1571856060048,1570206360590,1569267060072,1567600200783,1565965800980,\n",
        "            1564618440716,1563282120008,1561045680206,1559133960526,1557427680731,1556198100070,\n",
        "            1554825000123,1552924740401,1551208620373,1550168460985,1549035480816,\n",
        "            1547746620251,1544817960808,1543529340295,1541717040221]\n",
        "\n",
        "csv_file = open('TheOnion.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['headlines', 'is_sarcastic'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l60JmxSKeKT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Onion_crawler(tag, ids, num_articles, label=1):\n",
        "  count = 0\n",
        "  for id in ids:\n",
        "    source = requests.get(\"https://www.theonion.com/c/{}?startTime={}\".format(tag,id)).text\n",
        "    soup = BeautifulSoup(source, 'lxml')\n",
        "    for article in soup.find_all('article'):\n",
        "      headline = article.h2.text\n",
        "      csv_writer.writerow([headline,1])\n",
        "      count += 1\n",
        "      if count == num_articles:\n",
        "        print(f\"Crawled {count} headlines from {tag} tag\")\n",
        "        return"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r58CVsSteKkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "36642811-af88-4b60-8bd6-2b9e40062e70"
      },
      "source": [
        "Onion_crawler(\"news-in-brief\", brief_ids, num_articles=750, label=1)\n",
        "Onion_crawler(\"news-in-photos\", photo_ids, num_articles=610, label=1)\n",
        "csv_file.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crawled 750 headlines from news-in-brief tag\n",
            "Crawled 610 headlines from news-in-photos tag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9DFwWPheOk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "52edd5cd-0fa6-49d5-cbbe-a9901d44843a"
      },
      "source": [
        "#đọc file TheOnion\n",
        "onion_df = pd.read_csv(\"TheOnion.csv\")\n",
        "onion_df.head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Drew Brees Admits He Doesn’t Trust Black Peopl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mark Zuckerberg Announces Virtual Roundtable W...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>‘This Face Will Be The Last Thing You See Befo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Primaried Steve King Glad He At Least Won’t Ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Police Defend Use Of Non-Lethal Rubber Tires O...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Democratic Leaders Announce That They’ve Learn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>‘She Needs A Bible Now, Fuckwad—Yes, It’s For ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Auntie Anne’s Breaks From Pack By Calling For ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Trump Throws Garbage Can Through McDonald’s Wi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Police Didn’t Spend Millions On Awesome Tank J...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines  is_sarcastic\n",
              "0  Drew Brees Admits He Doesn’t Trust Black Peopl...             1\n",
              "1  Mark Zuckerberg Announces Virtual Roundtable W...             1\n",
              "2  ‘This Face Will Be The Last Thing You See Befo...             1\n",
              "3  Primaried Steve King Glad He At Least Won’t Ha...             1\n",
              "4  Police Defend Use Of Non-Lethal Rubber Tires O...             1\n",
              "5  Democratic Leaders Announce That They’ve Learn...             1\n",
              "6  ‘She Needs A Bible Now, Fuckwad—Yes, It’s For ...             1\n",
              "7  Auntie Anne’s Breaks From Pack By Calling For ...             1\n",
              "8  Trump Throws Garbage Can Through McDonald’s Wi...             1\n",
              "9  Police Didn’t Spend Millions On Awesome Tank J...             1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaNpnUZPeSQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Chuyển đổi từ file csv sang file json\n",
        "import csv\n",
        "import json\n",
        "\n",
        "csvfile = open('/content/TheOnion.csv', 'r')\n",
        "jsonfile = open('Mydata_TheOnion.json', 'w')\n",
        "\n",
        "fieldnames = (\"headlines\", \"is_sarcastic\")\n",
        "reader = csv.DictReader( csvfile, fieldnames)\n",
        "for row in reader:\n",
        "    json.dump(row, jsonfile)\n",
        "    jsonfile.write('\\n')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDzPhV2EhBc2",
        "colab_type": "text"
      },
      "source": [
        "Em trộn 2 file json của 2 trang Huffpost và TheOnion vào file 2000_test_headlines.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X-f_PDpP5qh",
        "colab_type": "text"
      },
      "source": [
        "### kết nối gg drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72LAdH5mfR9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ef5f1fe7-586b-40e1-9f53-4abecf18fc66"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q2fIQ1sP-zl",
        "colab_type": "text"
      },
      "source": [
        "### import thư viện cần thiết "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk1hffyZfvLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd, numpy as np, re, time\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5BmifI8QDZc",
        "colab_type": "text"
      },
      "source": [
        "#đọc file json của kaggle (file train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8v71rOwf4BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading train data from json file\n",
        "data_train = pd.read_json(\"/content/drive/My Drive/hoctap/sarcasm/Sarcasm_Headlines_Dataset.json\", lines = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEZFbgmhQJIM",
        "colab_type": "text"
      },
      "source": [
        "#kiểm tra "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nEN0kLPga8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "0f909bb0-23ef-4282-8ddb-a9d567752480"
      },
      "source": [
        "print(data_train.isnull().any(axis = 0))\n",
        "# Output :-\n",
        "# ----------------------\n",
        "# article_link    False\n",
        "# headline        False\n",
        "# is_sarcastic    False\n",
        "# dtype: bool\n",
        "# ----------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "article_link    False\n",
            "headline        False\n",
            "is_sarcastic    False\n",
            "dtype: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AacXIU2ThL9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Relacing special symbols and digits in headline column\n",
        "# re stands for Regular Expression\n",
        "#LOẠI BỎ CÁC KÍ TỰ ĐẶC BIỆT KHÔNG CẦN THIẾT TRÊN TẬP TEST \n",
        "data_train['headline'] = data_train['headline'].apply(lambda s : re.sub('[^a-zA-Z]', ' ', s))\n",
        "# getting features and labels\n",
        "features_train = data_train['headline']\n",
        "labels_train = data_train['is_sarcastic']\n",
        "# Stemming our data\n",
        "ps = PorterStemmer()\n",
        "features_train = features_train.apply(lambda x: x.split())\n",
        "features_train = features_train.apply(lambda x : \" \".join([ps.stem(word) for word in x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYZp_242hTyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizing the data with maximum of 5000 features\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(max_features = 5000)\n",
        "features_train = list(features_train)\n",
        "features_train = tv.fit_transform(features_train).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3blFw2uIDRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading test data from THUNGADETHUONG WITH 2000 headlines\n",
        "testdata = pd.read_json(\"/content/drive/My Drive/hoctap/sarcasm/2000_test_headlines.json\", lines = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fhmKbFbK3Eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "9662ae01-80e1-42fc-bc18-83704829933d"
      },
      "source": [
        "print(testdata.isnull().any(axis = 0))\n",
        "#KIỂM TRA TẬP TEST "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "article_link     True\n",
            "headlines       False\n",
            "is_sarcastic    False\n",
            "dtype: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5usAJiPI3sO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Relacing special symbols and digits in headline column\n",
        "# re stands for Regular Expression\n",
        "#LOẠI BỎ CÁC KÍ TỰ ĐẶC BIỆT KHÔNG CẦN THIẾT TRÊN TẬP TEST \n",
        "testdata['headlines'] = testdata['headlines'].apply(lambda s : re.sub('[^a-zA-Z]', ' ', s))\n",
        "# getting features and labels\n",
        "#LẤY FEATURES VÀ LABEL CHO TẬP TEST \n",
        "features_test = testdata['headlines']\n",
        "labels_test = testdata['is_sarcastic']\n",
        "# Stemming our data\n",
        "ps = PorterStemmer()\n",
        "features_test = features_test.apply(lambda x: x.split())\n",
        "features_test = features_test.apply(lambda x : \" \".join([ps.stem(word) for word in x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwIANTLNTL3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature extraction \n",
        "# Khai báo TfidfVectorizer để biểu diễn dữ liệu train dưới dạng vector và tạo một từ điển từ dữ liệu train.\n",
        "# Sử dụng lại từ điển của dữ liệu trên để biểu diễn dữ liệu test dưới dạng vector\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(max_features = 5000)\n",
        "features_test = list(features_test)\n",
        "features_test = tv.fit_transform(features_test).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTeJYtsahmKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "990bff49-d792-42e0-cc0c-a5ede24c556a"
      },
      "source": [
        "# model 1:-\n",
        "# Using linear support vector classifier\n",
        "lsvc = LinearSVC()\n",
        "# training the model\n",
        "lsvc.fit(features_train, labels_train)\n",
        "# getting the score of train and test data\n",
        "print(lsvc.score(features_train, labels_train))\n",
        "print(lsvc.score(features_test, labels_test))   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9075966902542214\n",
            "0.5158730158730159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94xvGMJzjbE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "c9666c3a-de28-4158-9602-8ea44eb212f0"
      },
      "source": [
        "# model 2:-\n",
        "# Using Gaussuan Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(features_train, labels_train)\n",
        "print('Using Gaussuan Naive Bayes')\n",
        "print(gnb.score(features_train, labels_train)) \n",
        "print(gnb.score(features_test, labels_test))   \n",
        "# model 3:-\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(features_train, labels_train)\n",
        "print('Logistic Regression')\n",
        "print(lr.score(features_train, labels_train))  \n",
        "print(lr.score(features_test, labels_test))   \n",
        "# model 4:-\n",
        "# Random Forest Classifier\n",
        "rfc = RandomForestClassifier(n_estimators = 10, random_state = 0)\n",
        "rfc.fit(features_train, labels_train)\n",
        "print('Random Forest Classifier')\n",
        "print(rfc.score(features_train, labels_train)) \n",
        "print(rfc.score(features_test, labels_test))   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Gaussuan Naive Bayes\n",
            "0.7844172376352541\n",
            "0.5104166666666666\n",
            "Logistic Regression\n",
            "0.880265079186791\n",
            "0.5054563492063492\n",
            "Random Forest Classifier\n",
            "0.9885431876895429\n",
            "0.5500992063492064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shCyEvfcT-h1",
        "colab_type": "text"
      },
      "source": [
        "### CHIA TẬP TRAIN VÀ TEST TRÊN TẬP DATA KAGGLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0rXeLuHhdnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features=features_train\n",
        "labels=labels_train\n",
        "#getting training and testing data\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = .05, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQhO1jGU1_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "4d92ece3-2e41-4490-c81a-a2f2c18a366f"
      },
      "source": [
        "# model 1:-\n",
        "# Using linear support vector classifier\n",
        "lsvc = LinearSVC()\n",
        "# training the model\n",
        "lsvc.fit(features_train, labels_train)\n",
        "# getting the score of train and test data\n",
        "print(' Using linear support vector classifier')\n",
        "print(lsvc.score(features_train, labels_train)) # 90.93\n",
        "print(lsvc.score(features_test, labels_test))   # 83.75\n",
        "# model 2:-\n",
        "# Using Gaussuan Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(features_train, labels_train)\n",
        "print('Using Gaussuan Naive Bayes')\n",
        "print(gnb.score(features_train, labels_train))  # 78.86\n",
        "print(gnb.score(features_test, labels_test))    # 73.80\n",
        "# model 3:-\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(features_train, labels_train)\n",
        "print('Logistic Regression')\n",
        "print(lr.score(features_train, labels_train))   # 88.16\n",
        "print(lr.score(features_test, labels_test))     # 83.08\n",
        "# model 4:-\n",
        "# Random Forest Classifier\n",
        "rfc = RandomForestClassifier(n_estimators = 10, random_state = 0)\n",
        "rfc.fit(features_train, labels_train)\n",
        "print('Random Forest Classifier')\n",
        "print(rfc.score(features_train, labels_train))  # 98.82\n",
        "print(rfc.score(features_test, labels_test))    # 79.71"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Using linear support vector classifier\n",
            "0.9093524612777362\n",
            "0.8375748502994012\n",
            "Using Gaussuan Naive Bayes\n",
            "0.7886335868836952\n",
            "0.7380239520958084\n",
            "Logistic Regression\n",
            "0.8816458440074094\n",
            "0.8308383233532934\n",
            "Random Forest Classifier\n",
            "0.9882946439128207\n",
            "0.7971556886227545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rtYehOQRCQd",
        "colab_type": "text"
      },
      "source": [
        "### Đối chiếu performance của model trên dataset đã cho và trên 2000 headine mới. Nêu nhận xét của nhóm về bài toán này.\n",
        "\n",
        "- Từ kết quả trên thì performance của model trên dataset đã cho có kết quả cao hơn 2000 headline mới.\n",
        "- Sử dụng bag of word không giúp hiểu nội dung headlines. Vì vậy cần tìm hiểu và test 1 số thuật toán khác như trong 1 bài báo sử dụng WORD2VEC MODEL và GloVe Embeddings để cải thiện performance hơn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyMxjTOWOE86",
        "colab_type": "text"
      },
      "source": [
        "###Mô tả cách dùng model đã train để viết một đoạn chương trình ngắn, thực hiện sacarsm detection cho một headline bất kỳ được nhập vào."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL1nL8VfO9Uc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "a49ee992-fa4d-4700-ee48-3aac99ff4507"
      },
      "source": [
        "import re\n",
        "def pre_process(headline): #preprecessing data\n",
        "\theadline = headline.lower()                                 \n",
        "\theadline = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", headline)\n",
        "\theadline = re.sub(\"(\\\\d|\\\\WZhe Yang)+\",\" \", headline)                   \n",
        "\treturn headline\n",
        "data_train['headline'] = data_train['headline'].apply(lambda x:pre_process(x))\n",
        "data_train['headline']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        former versace store clerk sues over secret  b...\n",
              "1        the  roseanne  revival catches up to our thorn...\n",
              "2        mom starting to fear son s web series closest ...\n",
              "3        boehner just wants wife to listen  not come up...\n",
              "4        j k  rowling wishes snape happy birthday in th...\n",
              "                               ...                        \n",
              "26704                 american politics in moral free fall\n",
              "26705                              america s best    hikes\n",
              "26706                                reparations and obama\n",
              "26707    israeli ban targeting boycott supporters raise...\n",
              "26708                    gourmet gifts for the foodie     \n",
              "Name: headline, Length: 26709, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhcR_0YpYP3H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "94a7c643-eec6-4e57-bc35-aff3319e66a8"
      },
      "source": [
        "headline = input(\"Enter headline: \")\n",
        "for cls in [LinearSVC,GaussianNB,LogisticRegression,RandomForestClassifier]:\n",
        "    print(cls.__class__.__name__, cls.predict(TfidfVectorizer.transform([pre_process(headline)])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter headline: a a \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-803ffecdc6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mheadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter headline: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'raw_documents'"
          ]
        }
      ]
    }
  ]
}