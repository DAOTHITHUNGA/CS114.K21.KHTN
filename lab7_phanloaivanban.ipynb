{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab7_phanloaivanban.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAOTHITHUNGA/CS114.K21.KHTN/blob/master/lab7_phanloaivanban.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DvhA3zgIfnQ",
        "outputId": "97dbd4b8-7244-474d-c07e-9c6916c6c445"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzUuOvCN_RM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.naive_bayes import MultinomialNB as MNB\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tarfile\n",
        "#from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaCHdjzi-G9w"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e2lTuGL_mCj"
      },
      "source": [
        "##From zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3mzd8BULkCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a298eddc-d351-47a9-885f-393b2aa68188"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/truyvan/20_newsgroups'\n",
        "def load_data(data_path):\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "    file_name = []\n",
        "    labels = os.listdir(data_path)\n",
        "    for label in labels:\n",
        "        print(f'Reading {label}...')\n",
        "        folder = os.path.join(data_path,label)\n",
        "        files = os.listdir(os.path.join(folder))\n",
        "        for i,file in enumerate(files):\n",
        "            with open(os.path.join(folder,file),errors ='ignore', encoding = 'utf-8') as data:\n",
        "                #text là đọc ghi lại giá trị của x x_data.append\n",
        "                text = data.read()\n",
        "                x_data.append(text)\n",
        "                #y chứa label \n",
        "                y_data.append(label)\n",
        "                file_name.append(file)\n",
        "                if (i+1)%100 ==0:\n",
        "                    print(f'\\tRead {i+1} files.')\n",
        "    return x_data, y_data, labels,file_name\n",
        "x_data,y_data,labels,file_name = load_data(data_path) \n",
        "print(len(x_data))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading rec.sport.hockey...\n",
            "\tRead 100 files.\n",
            "\tRead 200 files.\n",
            "\tRead 300 files.\n",
            "\tRead 400 files.\n",
            "\tRead 500 files.\n",
            "\tRead 600 files.\n",
            "\tRead 700 files.\n",
            "\tRead 800 files.\n",
            "\tRead 900 files.\n",
            "\tRead 1000 files.\n",
            "Reading sci.crypt...\n",
            "\tRead 100 files.\n",
            "\tRead 200 files.\n",
            "\tRead 300 files.\n",
            "\tRead 400 files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc4oBT7x8Ygh"
      },
      "source": [
        "##preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im79rxrA4fT8"
      },
      "source": [
        "def preprocessing(text_data):\n",
        "\n",
        "    ps = PorterStemmer()\n",
        "    res = []\n",
        "    for i in range(len(text_data)):\n",
        "        text = text_data[i]\n",
        "        text = re.sub('[^a-z]', ' ', text_data[i].lower())\n",
        "        #text = text.apply(lambda s : re.sub('[^a-zA-Z]', ' ', s))\n",
        "        res.append( ' '.join([ps.stem(word) for word in text.split()]))\n",
        "        #text = text.apply(lambda x: x.split())\n",
        "        #text = text.apply(lambda x : \" \".join([ps.stem(word) for word in x]))\n",
        "    return res\n",
        "def vectorize(train,test):\n",
        "    vectorizer1 = CountVectorizer(analyzer='word',ngram_range=(1,1), max_features=10000) \n",
        "    features_train = vectorizer1.fit_transform(train)\n",
        "\n",
        "    vectorizer2 = CountVectorizer(vocabulary=vectorizer1.vocabulary_,ngram_range=(1,1)) \n",
        "    features_test = vectorizer2.fit_transform(test)\n",
        "    return features_train, features_test\n",
        "\n",
        "#import TfidfVectorizer\n",
        "#Một thuật toán rất phổ biến để chuyển đổi văn bản thành một đại diện có ý nghĩa của các con số được biểu diễn dưới dạng vector\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#tv = TfidfVectorizer()\n",
        "#tv.fit(headline_kaggle)\n",
        "#bag_of_word  = tv.vocabulary_\n",
        "#in ra so tu moi chua trong thu vien \n",
        "#print(bag_of_word)\n",
        "#print(len(bag_of_word))\n",
        "#def feature_extract(headlines):\n",
        "#   tv = TfidfVectorizer(vocabulary=bag_of_word, max_features=5000)\n",
        "#  headlines = list(headlines)\n",
        "#    features = tv.fit_transform(headlines).toarray()\n",
        "#    return features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl17eFnzH1ro"
      },
      "source": [
        "SAVE FEATURES + LABELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSSS4dJoHyfj"
      },
      "source": [
        "# Đường dẫn chứa các file lưu\n",
        "%cd '/content/drive/MyDrive/truyvan/'\n",
        "\n",
        "#FEATURES\n",
        "# Tên file lưu là 20NGs_Feature\n",
        "featuresPickle = open('20NGs_Feature', 'wb') \n",
        "# Lưu x_data vào file vừa đặt tên\n",
        "pickle.dump(x_data, featuresPickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00u8TGpkcbQo"
      },
      "source": [
        "#LABELS\n",
        "%cd '/content/drive/MyDrive/truyvan/'\n",
        "labelsPickle = open('labels', 'wb')\n",
        "pickle.dump(y_data, labelsPickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgamKH7eJpDi"
      },
      "source": [
        "LOAD SAVED FEATURES + LABELS FILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASZS7h5xJ22Y"
      },
      "source": [
        "x_data=pickle.load(open('/content/drive/MyDrive/truyvan/20NGs_Feature','rb'))\n",
        "y_data=pickle.load(open('/content/drive/MyDrive/truyvan/labels','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mytTCQzOaV1L"
      },
      "source": [
        "print(len(x_data),len(y_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJyLAuvxJ2Db"
      },
      "source": [
        "SPLIT DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_sqAPDT8Ygi"
      },
      "source": [
        "x_data = preprocessing(x_data)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25,shuffle= True, random_state=56)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CohFCvd08Ygi"
      },
      "source": [
        "def training(x_train,y_train,x_test,y_test, n_samples = 10000, max_features = 2000):\n",
        "    samples = random.sample(range(len(x_train)),n_samples)\n",
        "    X_train = [x_train[i] for i in samples]\n",
        "    Y_train = [y_train[i] for i in samples]\n",
        "\n",
        "    naiB = MNB() #Naive Bayes\n",
        "    knn = KNC() #KNeighborsClassifier\n",
        "#train model \n",
        "    vectorizer1 = CountVectorizer(analyzer='word',ngram_range=(1,1), max_features=max_features) \n",
        "    features_train = vectorizer1.fit_transform(X_train) \n",
        "#test model \n",
        "    vectorizer2 = CountVectorizer(vocabulary=vectorizer1.vocabulary_,ngram_range=(1,1)) \n",
        "    features_test = vectorizer2.fit_transform(x_test)\n",
        "\n",
        "    naiB.fit(features_train,Y_train)\n",
        "    naiB_train = naiB.score(features_train,Y_train)\n",
        "    naiB_score = naiB.score(features_test,y_test)\n",
        "    print(n_samples)\n",
        "    print(\"MNB: \",naiB_score)\n",
        "\n",
        "    knn.fit(features_train,Y_train)\n",
        "    knn_train = knn.score(features_train,Y_train)\n",
        "    knn_score = knn.score(features_test,y_test)\n",
        "    print(\"KNN: \",knn_score)\n",
        "\n",
        "\n",
        "    return mnb_train, naiB_score, knn_train, knn_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT4xgA6b8Ygi"
      },
      "source": [
        "#plot result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyOtOxCt8Ygi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e658b9-903e-45c6-b665-5e2c3ea15d65"
      },
      "source": [
        "list_n_task = [1000, 3000, 5000, 7000, 9000, 11000,13000, len(x_train)]\n",
        "mnb_train = []\n",
        "mnb_test  = []\n",
        "knn_train = []\n",
        "knn_test  = []\n",
        "\n",
        "for n_task in list_n_task:\n",
        "    res = training(x_train,y_train,x_test,y_test,n_task)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "MNB:  0.6758\n",
            "KNN:  0.3222\n",
            "3000\n",
            "MNB:  0.7622\n",
            "KNN:  0.4502\n",
            "5000\n",
            "MNB:  0.78\n",
            "KNN:  0.4816\n",
            "7000\n",
            "MNB:  0.7962\n",
            "KNN:  0.522\n",
            "9000\n",
            "MNB:  0.799\n",
            "KNN:  0.5548\n",
            "11000\n",
            "MNB:  0.7958\n",
            "KNN:  0.578\n",
            "13000\n",
            "MNB:  0.8012\n",
            "KNN:  0.602\n",
            "14997\n",
            "MNB:  0.8056\n",
            "KNN:  0.6094\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}